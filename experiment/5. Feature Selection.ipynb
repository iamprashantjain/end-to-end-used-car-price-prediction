{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a62cbca1",
   "metadata": {},
   "source": [
    "### Importing & Remove completly duplicate columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8d038ef9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Reading Excel files into DataFrames\n",
    "df_mean_mode_ohe = pd.read_excel(r\"H:\\CampusX_DS\\week43 - My Projects Aug 2024\\used_car_price_prediction\\Used-Car-Price-Prediction\\src\\notebook\\data\\model_ready_data\\df_mean_mode_ohe.xlsx\")\n",
    "df_median_mode_ohe = pd.read_excel(r\"H:\\CampusX_DS\\week43 - My Projects Aug 2024\\used_car_price_prediction\\Used-Car-Price-Prediction\\src\\notebook\\data\\model_ready_data\\df_median_mode_ohe.xlsx\")\n",
    "df_knn_imputed_ohe = pd.read_excel(r\"H:\\CampusX_DS\\week43 - My Projects Aug 2024\\used_car_price_prediction\\Used-Car-Price-Prediction\\src\\notebook\\data\\model_ready_data\\df_knn_imputed_ohe.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8883ec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_mode_ohe.drop(columns=['content.appointmentId'], inplace=True)\n",
    "df_median_mode_ohe.drop(columns=['content.appointmentId'], inplace=True)\n",
    "df_knn_imputed_ohe.drop(columns=['content.appointmentId'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "feba462c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2719, 816) (2719, 816) (2719, 823)\n"
     ]
    }
   ],
   "source": [
    "print(df_mean_mode_ohe.shape, df_median_mode_ohe.shape, df_knn_imputed_ohe.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab02187d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Identify and remove completely duplicate columns\n",
    "df_mean_mode_ohe = df_mean_mode_ohe.loc[:, ~df_mean_mode_ohe.T.duplicated()]\n",
    "df_median_mode_ohe = df_median_mode_ohe.loc[:, ~df_median_mode_ohe.T.duplicated()]\n",
    "df_knn_imputed_ohe = df_knn_imputed_ohe.loc[:, ~df_knn_imputed_ohe.T.duplicated()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "54a6fd36",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2719, 803) (2719, 803) (2719, 811)\n"
     ]
    }
   ],
   "source": [
    "print(df_mean_mode_ohe.shape, df_median_mode_ohe.shape, df_knn_imputed_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa3f2cfc",
   "metadata": {},
   "source": [
    "### Remove model & variant to simplify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d0c695d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mean_mode_ohe = df_mean_mode_ohe.loc[:, ~df_mean_mode_ohe.columns.str.startswith(\"content.model_\")]\n",
    "df_median_mode_ohe = df_median_mode_ohe.loc[:, ~df_median_mode_ohe.columns.str.startswith(\"content.model_\")]\n",
    "df_knn_imputed_ohe = df_knn_imputed_ohe.loc[:, ~df_knn_imputed_ohe.columns.str.startswith(\"content.model_\")]\n",
    "\n",
    "df_mean_mode_ohe = df_mean_mode_ohe.loc[:, ~df_mean_mode_ohe.columns.str.startswith(\"content.variant_\")]\n",
    "df_median_mode_ohe = df_median_mode_ohe.loc[:, ~df_median_mode_ohe.columns.str.startswith(\"content.variant_\")]\n",
    "df_knn_imputed_ohe = df_knn_imputed_ohe.loc[:, ~df_knn_imputed_ohe.columns.str.startswith(\"content.variant_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c894c5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2719, 127) (2719, 127) (2719, 135)\n"
     ]
    }
   ],
   "source": [
    "print(df_mean_mode_ohe.shape, df_median_mode_ohe.shape, df_knn_imputed_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99919400",
   "metadata": {},
   "source": [
    "### Find Base Accuracy of each dataset to compare (Apply All Regression Algorithms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "419fbb06",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Applying models to dataset 1\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001714 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1278\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001785 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001029 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1295\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 108\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001491 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002210 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1287\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 107\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001198 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1278\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002121 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1297\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001265 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1295\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 108\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001793 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1293\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001056 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1287\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 107\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n",
      "Error occurred while processing model Keras Neural Network: 'function' object has no attribute 'predict'\n",
      "Error occurred while processing model Bayesian Linear Regression: shapes (2719,127) and (1,127) not aligned: 127 (dim 1) != 1 (dim 0)\n",
      "Applying models to dataset 2\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001190 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001266 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1294\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001141 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1292\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 108\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001497 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1290\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002026 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 107\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001902 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1275\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001226 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1294\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001204 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1292\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 108\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001732 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1290\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 109\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001193 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1284\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 107\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Error occurred while processing model Keras Neural Network: 'function' object has no attribute 'predict'\n",
      "Error occurred while processing model Bayesian Linear Regression: shapes (2719,127) and (1,127) not aligned: 127 (dim 1) != 1 (dim 0)\n",
      "Applying models to dataset 3\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001455 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001393 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1613\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001499 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001966 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1597\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001683 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 115\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001314 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1589\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 953609.615632\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002117 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1613\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 963839.552184\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001839 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 116\n",
      "[LightGBM] [Info] Start training from score 957683.996322\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.001565 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1597\n",
      "[LightGBM] [Info] Number of data points in the train set: 2175, number of used features: 117\n",
      "[LightGBM] [Info] Start training from score 942238.014253\n",
      "[LightGBM] [Info] Auto-choosing row-wise multi-threading, the overhead of testing was 0.002066 seconds.\n",
      "You can set `force_row_wise=true` to remove the overhead.\n",
      "And if memory is not enough, you can set `force_col_wise=true`.\n",
      "[LightGBM] [Info] Total Bins 1604\n",
      "[LightGBM] [Info] Number of data points in the train set: 2176, number of used features: 115\n",
      "[LightGBM] [Info] Start training from score 966774.016085\n",
      "Error occurred while processing model Keras Neural Network: 'function' object has no attribute 'predict'\n",
      "Error occurred while processing model Bayesian Linear Regression: shapes (2719,135) and (1,135) not aligned: 135 (dim 1) != 1 (dim 0)\n",
      "      Dataset                     Algorithm            R²           MSE\n",
      "11  dataset_1                      CatBoost  9.681138e-01  5.893417e+09\n",
      "23  dataset_2                      CatBoost  9.680965e-01  5.896297e+09\n",
      "35  dataset_3                      CatBoost  9.680083e-01  5.923517e+09\n",
      "10  dataset_1                      LightGBM  9.655596e-01  6.374923e+09\n",
      "34  dataset_3                      LightGBM  9.654988e-01  6.389698e+09\n",
      "22  dataset_2                      LightGBM  9.653662e-01  6.410910e+09\n",
      "33  dataset_3                       XGBoost  9.617704e-01  7.070225e+09\n",
      "9   dataset_1                       XGBoost  9.598924e-01  7.423472e+09\n",
      "17  dataset_2                 Random Forest  9.593685e-01  7.618678e+09\n",
      "21  dataset_2                       XGBoost  9.591634e-01  7.560086e+09\n",
      "5   dataset_1                 Random Forest  9.588385e-01  7.479124e+09\n",
      "29  dataset_3                 Random Forest  9.584382e-01  7.718233e+09\n",
      "13  dataset_2              Ridge Regression  9.571813e-01  7.868884e+09\n",
      "1   dataset_1              Ridge Regression  9.569504e-01  7.912027e+09\n",
      "14  dataset_2              Lasso Regression  9.568814e-01  7.926037e+09\n",
      "2   dataset_1              Lasso Regression  9.566558e-01  7.968461e+09\n",
      "25  dataset_3              Ridge Regression  9.563603e-01  8.025394e+09\n",
      "26  dataset_3              Lasso Regression  9.561494e-01  8.064268e+09\n",
      "15  dataset_2         ElasticNet Regression  9.490826e-01  9.359149e+09\n",
      "3   dataset_1         ElasticNet Regression  9.489228e-01  9.388434e+09\n",
      "27  dataset_3         ElasticNet Regression  9.483064e-01  9.501439e+09\n",
      "28  dataset_3                 Decision Tree  9.242949e-01  1.320429e+10\n",
      "16  dataset_2                 Decision Tree  9.212953e-01  1.602265e+10\n",
      "19  dataset_2           K-Nearest Neighbors  9.210163e-01  1.455207e+10\n",
      "7   dataset_1           K-Nearest Neighbors  9.206186e-01  1.462459e+10\n",
      "31  dataset_3           K-Nearest Neighbors  9.195195e-01  1.479275e+10\n",
      "4   dataset_1                 Decision Tree  9.112422e-01  1.710545e+10\n",
      "30  dataset_3  Support Vector Machine (SVR) -8.181666e-02  1.981596e+11\n",
      "6   dataset_1  Support Vector Machine (SVR) -8.183343e-02  1.981630e+11\n",
      "18  dataset_2  Support Vector Machine (SVR) -8.183987e-02  1.981643e+11\n",
      "20  dataset_2              Gaussian Process -5.026382e+00  1.098694e+12\n",
      "8   dataset_1              Gaussian Process -5.026433e+00  1.098703e+12\n",
      "32  dataset_3              Gaussian Process -5.028673e+00  1.099113e+12\n",
      "24  dataset_3             Linear Regression -3.016509e+21  5.142461e+32\n",
      "0   dataset_1             Linear Regression -6.826734e+22  1.235921e+34\n",
      "12  dataset_2             Linear Regression -1.369971e+23  2.485820e+34\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "import xgboost as xgb\n",
    "import lightgbm as lgb\n",
    "from catboost import CatBoostRegressor\n",
    "import statsmodels.api as sm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "import warnings; warnings.filterwarnings('ignore')\n",
    "\n",
    "# Define a list of algorithms grouped as mentioned\n",
    "linear_regression_models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(),\n",
    "    'Lasso Regression': Lasso(),\n",
    "    'ElasticNet Regression': ElasticNet()\n",
    "}\n",
    "\n",
    "non_linear_regression_models = {\n",
    "    'Decision Tree': DecisionTreeRegressor(),\n",
    "    'Random Forest': RandomForestRegressor(),\n",
    "    'Support Vector Machine (SVR)': SVR(),\n",
    "    'K-Nearest Neighbors': KNeighborsRegressor(),\n",
    "    'Gaussian Process': GaussianProcessRegressor()\n",
    "}\n",
    "\n",
    "advanced_regression_models = {\n",
    "    'XGBoost': xgb.XGBRegressor(objective='reg:squarederror'),\n",
    "    'LightGBM': lgb.LGBMRegressor(),\n",
    "    'CatBoost': CatBoostRegressor(learning_rate=0.1, iterations=500, depth=6, silent=True)\n",
    "}\n",
    "\n",
    "# Deep Learning Model\n",
    "def build_keras_nn(X_train, y_train):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, input_dim=X_train.shape[1], activation='relu'))\n",
    "    model.add(Dense(32, activation='relu'))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "# Bayesian Regression Model\n",
    "bayesian_regression_models = {\n",
    "    'Bayesian Linear Regression': sm.OLS\n",
    "}\n",
    "\n",
    "# Combine all models into one dictionary\n",
    "all_models = {\n",
    "    **linear_regression_models,\n",
    "    **non_linear_regression_models,\n",
    "    **advanced_regression_models,\n",
    "    'Keras Neural Network': build_keras_nn,\n",
    "    **bayesian_regression_models\n",
    "}\n",
    "\n",
    "# Function to apply all regression models to a dataset using cross-validation\n",
    "def apply_regression_models(data, target_column):\n",
    "    X = data.drop(target_column, axis=1)\n",
    "    y = data[target_column]\n",
    "    \n",
    "    # Preprocessing: Convert categorical variables to dummy variables, and scale features\n",
    "    X = pd.get_dummies(X)  # Convert categorical variables to dummy variables\n",
    "    X = StandardScaler().fit_transform(X)  # Standard scaling\n",
    "    \n",
    "    # Dictionary to hold results\n",
    "    results = []\n",
    "    \n",
    "    # Apply each model using cross-validation\n",
    "    for name, model in all_models.items():\n",
    "        try:\n",
    "            if name == 'Keras Neural Network':\n",
    "                # Keras model needs to be fit on the entire dataset\n",
    "                build_keras_nn(X, y)\n",
    "                y_pred = model.predict(X)\n",
    "                mse = mean_squared_error(y, y_pred)\n",
    "                r2 = r2_score(y, y_pred)\n",
    "            elif name == 'Bayesian Linear Regression':\n",
    "                X_with_const = sm.add_constant(X)\n",
    "                model_ols = model(X_with_const, y).fit()\n",
    "                y_pred = model_ols.predict(X_with_const)\n",
    "                mse = mean_squared_error(y, y_pred)\n",
    "                r2 = r2_score(y, y_pred)\n",
    "            else:\n",
    "                # Use cross-validation\n",
    "                scores = cross_val_score(model, X, y, scoring='r2', cv=5)\n",
    "                r2 = np.mean(scores)\n",
    "                mse = -np.mean(cross_val_score(model, X, y, scoring='neg_mean_squared_error', cv=5))\n",
    "            \n",
    "            # Store result\n",
    "            results.append({\n",
    "                'Algorithm': name,\n",
    "                'MSE': mse,\n",
    "                'R²': r2\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"Error occurred while processing model {name}: {e}\")\n",
    "    \n",
    "    # Return the results as a DataFrame, if any results exist\n",
    "    if results:\n",
    "        return pd.DataFrame(results)\n",
    "    else:\n",
    "        return pd.DataFrame()  # Return an empty DataFrame if no results\n",
    "\n",
    "# List of dataframes (df1, df2, df3)\n",
    "datasets = [df_mean_mode_ohe, df_median_mode_ohe, df_knn_imputed_ohe]\n",
    "\n",
    "# Loop over datasets\n",
    "results_all_datasets = []\n",
    "\n",
    "for idx, dataset in enumerate(datasets, start=1):\n",
    "    try:\n",
    "        print(f\"Applying models to dataset {idx}\")\n",
    "        # Apply regression models on the current dataset\n",
    "        target_column = 'content.onRoadPrice'\n",
    "        results_df = apply_regression_models(dataset, target_column)\n",
    "\n",
    "        # Check if results_df is not empty before appending\n",
    "        if not results_df.empty:\n",
    "            # Add dataset identifier to the results\n",
    "            results_df['Dataset'] = f'dataset_{idx}'\n",
    "            # Append results to the final list\n",
    "            results_all_datasets.append(results_df)\n",
    "        else:\n",
    "            print(f\"No results for dataset {idx}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error applying models to dataset {idx}: {e}\")\n",
    "\n",
    "# Check if there are any results to concatenate\n",
    "if results_all_datasets:\n",
    "    # Combine results of all datasets into one DataFrame\n",
    "    final_results = pd.concat(results_all_datasets, ignore_index=True)\n",
    "\n",
    "    # Sort the results by R² (or MSE, depending on your preference)\n",
    "    final_results_sorted = final_results.sort_values(by='R²', ascending=False)\n",
    "\n",
    "    # Display the sorted results, including the dataset name\n",
    "    print(final_results_sorted[['Dataset', 'Algorithm', 'R²', 'MSE']])\n",
    "else:\n",
    "    print(\"No results to concatenate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bc86da5",
   "metadata": {},
   "source": [
    "### Scaling data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ce59b2af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def scaling_dataframe(df, target_col):\n",
    "    # Separate features and target\n",
    "    X = df.drop(target_col, axis=1)\n",
    "    y = df[target_col]\n",
    "\n",
    "    # Scale the features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Create a new DataFrame with scaled features\n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=X.columns)\n",
    "    df_scaled[target_col] = y.values\n",
    "\n",
    "    return df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0ac52c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_dfs = []\n",
    "\n",
    "for i in datasets:\n",
    "    scaled_dfs.append(scaling_dataframe(i, \"content.onRoadPrice\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3e1fd8",
   "metadata": {},
   "source": [
    "### Filter Method 1: Corelation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c6377df2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_correlation_filter(df, target_column, threshold=0.5):\n",
    "    if target_column not in df.columns:\n",
    "        raise ValueError(f\"Target column '{target_column}' not found in the DataFrame.\")\n",
    "    \n",
    "    # Calculate the correlation matrix\n",
    "    correlation_matrix = df.corr()\n",
    "    \n",
    "    # Get the absolute correlation values with respect to the target column\n",
    "    correlations = correlation_matrix[target_column].abs()\n",
    "    \n",
    "    # Select features that have a correlation higher than the specified threshold\n",
    "    relevant_features = correlations[correlations > threshold].index.tolist()\n",
    "    \n",
    "    # Create a new DataFrame with the selected features\n",
    "    filtered_df = df[relevant_features]\n",
    "    \n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3308fe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_dfs = []\n",
    "target_column = \"content.onRoadPrice\"\n",
    "\n",
    "for i in scaled_dfs:\n",
    "    corr_dfs.append(apply_correlation_filter(i, target_column, threshold=0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "09ffa36c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Airbags',\n",
       " 'NumberOfSpeakers',\n",
       " 'Displacementcc',\n",
       " 'GearBoxNumberOfGears',\n",
       " 'NumberOfDiscBrakes',\n",
       " 'Widthmm',\n",
       " 'Lengthmm',\n",
       " 'WheelBasemm',\n",
       " 'FueltankCapacitylitres',\n",
       " 'MaxPowerbhp',\n",
       " 'MaxTorqueNm',\n",
       " 'content.bodyType_SUV',\n",
       " 'SeatUpholstery_Synthetic Leather',\n",
       " 'HeadlampLensType_Projector Beam',\n",
       " 'RimTypeFrontWheels_Steel',\n",
       " 'SmartCardSmartKey_1',\n",
       " 'AmbientLighting_1',\n",
       " 'SunroofMoonroof_1',\n",
       " 'DriverSeatAdjustmentElectric_1',\n",
       " 'content.onRoadPrice']"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dfs[0].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf6eea6",
   "metadata": {},
   "source": [
    "### Filter method 2: Apply f-regression & chi-square "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "3a418d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, chi2\n",
    "\n",
    "def select_features(data: pd.DataFrame, target_col: str, numerical_cols: list, k_num: int = 'all', k_cat: int = 'all') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Select features from the dataset using SelectKBest for numerical and categorical features,\n",
    "    and return a filtered DataFrame with selected features and the target.\n",
    "\n",
    "    Parameters:\n",
    "        data (pd.DataFrame): DataFrame containing feature variables and target.\n",
    "        target_col (str): The name of the target variable column.\n",
    "        numerical_cols (list): List of numerical column names.\n",
    "        k_num (int or str): Number of top numerical features to select; 'all' to select all.\n",
    "        k_cat (int or str): Number of top categorical features to select; 'all' to select all.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Filtered DataFrame with selected features and the target column.\n",
    "    \"\"\"\n",
    "    # Separate features and target\n",
    "    X = data.drop(columns=[target_col])\n",
    "    y = data[target_col]\n",
    "\n",
    "    # Step 1: Validate numerical columns\n",
    "    numerical_cols = [col for col in numerical_cols if col in X.columns]\n",
    "    \n",
    "    # Step 2: Separate Numerical and Categorical Features\n",
    "    X_numerical = X[numerical_cols]\n",
    "    X_categorical = X.drop(columns=numerical_cols).select_dtypes(include=['object'])\n",
    "\n",
    "    # Step 3: One-Hot Encoding for Categorical Features\n",
    "    if not X_categorical.empty:\n",
    "        X_encoded = pd.get_dummies(X_categorical, drop_first=True)\n",
    "    else:\n",
    "        X_encoded = pd.DataFrame()  # Empty DataFrame if no categorical features\n",
    "\n",
    "    # Step 4: Apply SelectKBest with f_regression for Numerical Features\n",
    "    if k_num == 'all':\n",
    "        k_num = X_numerical.shape[1]\n",
    "    selector_num = SelectKBest(score_func=f_regression, k=k_num)\n",
    "    selector_num.fit(X_numerical, y)\n",
    "    numerical_features = X_numerical.columns[selector_num.get_support()].tolist()\n",
    "\n",
    "    # Step 5: Apply SelectKBest with chi2 for Categorical Features\n",
    "    if not X_encoded.empty:\n",
    "        if k_cat == 'all':\n",
    "            k_cat = X_encoded.shape[1]\n",
    "        selector_cat = SelectKBest(score_func=chi2, k=k_cat)\n",
    "        selector_cat.fit(X_encoded, y)\n",
    "        categorical_features = X_encoded.columns[selector_cat.get_support()].tolist()\n",
    "    else:\n",
    "        categorical_features = []  # No categorical features to select\n",
    "\n",
    "    # Step 6: Combine the Selected Features\n",
    "    final_features = numerical_features + categorical_features\n",
    "\n",
    "    # Create a filtered DataFrame with selected features and the target column\n",
    "    filtered_df = data[final_features + [target_col]]\n",
    "\n",
    "    return filtered_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "7b388db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_col = [\n",
    "    \"content.year\", \"content.ownerNumber\", \"content.odometerReading\",\n",
    "    \"content.onRoadPrice\", \"Airbags\", \"NumberOfSpeakers\", \"Displacementcc\",\n",
    "    \"GearBoxNumberOfGears\", \"NumberOfDiscBrakes\", \"GroundClearancemm\",\n",
    "    \"SeatingCapacity\", \"Bootspacelitres\", \"Widthmm\", \"Lengthmm\",\n",
    "    \"WheelBasemm\", \"FueltankCapacitylitres\", \"MaxPowerbhp\",\n",
    "    \"MaxPowerrpm\", \"MaxTorqueNm\", \"defects\", \"repainted\",\n",
    "    \"MultifunctionDisplayScreenSizein\", \"EntertainmentDisplayScreenSizein\",\n",
    "    \"NCAPRating\"\n",
    "]\n",
    "\n",
    "kbest_dfs = []\n",
    "target_column = \"content.onRoadPrice\"\n",
    "\n",
    "for i in scaled_dfs:\n",
    "    filtered_df = select_features(i, target_column, numerical_col, k_num=20, k_cat=20)\n",
    "    kbest_dfs.append(filtered_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "9f37344d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content.year',\n",
       " 'Airbags',\n",
       " 'NumberOfSpeakers',\n",
       " 'Displacementcc',\n",
       " 'GearBoxNumberOfGears',\n",
       " 'NumberOfDiscBrakes',\n",
       " 'GroundClearancemm',\n",
       " 'SeatingCapacity',\n",
       " 'Bootspacelitres',\n",
       " 'Widthmm',\n",
       " 'Lengthmm',\n",
       " 'WheelBasemm',\n",
       " 'FueltankCapacitylitres',\n",
       " 'MaxPowerbhp',\n",
       " 'MaxPowerrpm',\n",
       " 'MaxTorqueNm',\n",
       " 'defects',\n",
       " 'repainted',\n",
       " 'MultifunctionDisplayScreenSizein',\n",
       " 'EntertainmentDisplayScreenSizein',\n",
       " 'content.onRoadPrice']"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "kbest_dfs[0].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add22fb8",
   "metadata": {},
   "source": [
    "### Step3: Tree-based Model Importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "97d3c00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def filter_features_by_importance(df, target_col, threshold=0.01):\n",
    "    # Split the data into features and target\n",
    "    X = df.drop(columns=[target_col])\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Split into training and testing sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    # Fit a Random Forest model\n",
    "    model = RandomForestClassifier(random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Get feature importances\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Create a DataFrame for feature importances\n",
    "    feature_importances = pd.DataFrame({\n",
    "        'Feature': X.columns,\n",
    "        'Importance': importances\n",
    "    })\n",
    "    \n",
    "    # Filter features based on the importance threshold\n",
    "    selected_features = feature_importances[feature_importances['Importance'] > threshold]['Feature']\n",
    "    \n",
    "    # Return the filtered DataFrame\n",
    "    return df[selected_features.to_list() + [target_col]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "b8f9c5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "featureImportance_dfs = []\n",
    "target_column = \"content.onRoadPrice\"\n",
    "\n",
    "for i in scaled_dfs:\n",
    "    filtered_df = filter_features_by_importance(i, target_column, threshold=0.01)\n",
    "    featureImportance_dfs.append(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "00ee5c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['content.year',\n",
       " 'content.ownerNumber',\n",
       " 'content.odometerReading',\n",
       " 'Airbags',\n",
       " 'NumberOfSpeakers',\n",
       " 'Displacementcc',\n",
       " 'GroundClearancemm',\n",
       " 'Bootspacelitres',\n",
       " 'Widthmm',\n",
       " 'Lengthmm',\n",
       " 'WheelBasemm',\n",
       " 'FueltankCapacitylitres',\n",
       " 'MaxPowerbhp',\n",
       " 'MaxPowerrpm',\n",
       " 'MaxTorqueNm',\n",
       " 'defects',\n",
       " 'repainted',\n",
       " 'content.fitnessUpto_months_remaining',\n",
       " 'content.insuranceExpiry_months_remaining',\n",
       " 'content.lastServicedAt_months_remaining',\n",
       " 'content.transmission_Manual',\n",
       " 'content.insuranceType_Comprehensive',\n",
       " 'content.insuranceType_Zero Depreciation',\n",
       " 'content.duplicateKey_1',\n",
       " 'Left Front Tyre_WARN',\n",
       " 'Right Front Tyre_WARN',\n",
       " 'Left Rear Tyre_WARN',\n",
       " 'Right Rear Tyre_WARN',\n",
       " 'Spare Tyre_WARN',\n",
       " 'car_state_DL',\n",
       " 'car_state_GJ',\n",
       " 'car_state_HR',\n",
       " 'car_state_KA',\n",
       " 'car_state_MH',\n",
       " 'car_state_TN',\n",
       " 'content.onRoadPrice']"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featureImportance_dfs[0].columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbaa063c",
   "metadata": {},
   "source": [
    "### Select Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "6142a20d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def union_of_features(dataframe_lists):\n",
    "    # Get the union of feature sets from multiple lists of DataFrames\n",
    "    union_features = set()\n",
    "    for dataframe_list in dataframe_lists:\n",
    "        for df in dataframe_list:\n",
    "            union_features.update(df.columns[:-1])  # Exclude target column\n",
    "    return union_features\n",
    "\n",
    "def top_n_features(dataframe_lists, n):\n",
    "    # Get the top N features from each DataFrame in the lists\n",
    "    top_features = set()\n",
    "    for dataframe_list in dataframe_lists:\n",
    "        for df in dataframe_list:\n",
    "            top_n = df.columns[:-1][:n]  # Exclude target column and take top N\n",
    "            top_features.update(top_n)\n",
    "    return top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "a0465001",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dataframe_lists = [featureImportance_dfs, kbest_dfs, corr_dfs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "a814c3e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get union of features\n",
    "union_features = union_of_features(all_dataframe_lists)\n",
    "\n",
    "# Get top N features (e.g., top 5 features)\n",
    "top_n = top_n_features(all_dataframe_lists, n=51)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "ce671224",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lengthmm', 'content.year', 'DriverSeatAdjustmentElectric_1', 'Right Rear Tyre_WARN', 'car_state_DL', 'content.transmission_Manual', 'EntertainmentDisplayScreenSizein', 'car_state_KA', 'SmartCardSmartKey_1', 'MaxPowerbhp', 'content.duplicateKey_1', 'ParkingAssistRear_Sensor & Camera', 'content.ownerNumber', 'Right Front Tyre_WARN', 'content.insuranceExpiry_months_remaining', 'MultifunctionDisplayScreenSizein', 'RimTypeFrontWheels_Steel', 'Bootspacelitres', 'Widthmm', 'Left Front Tyre_WARN', 'MaxTorqueNm', 'AmbientLighting_1', 'GearBoxNumberOfGears', 'SunroofMoonroof_1', 'SeatingCapacity', 'Left Rear Tyre_WARN', 'content.odometerReading', 'MaxPowerrpm', 'Airbags', 'Displacementcc', 'repainted', 'content.fitnessUpto_months_remaining', 'content.insuranceType_Comprehensive', 'Spare Tyre_WARN', 'SunroofType_nan', 'NumberOfSpeakers', 'car_state_HR', 'WheelBasemm', 'car_state_TN', 'NumberOfDiscBrakes', 'content.bodyType_SUV', 'SeatUpholstery_Synthetic Leather', 'HeadlampLensType_Projector Beam', 'content.lastServicedAt_months_remaining', 'GroundClearancemm', 'content.insuranceType_Zero Depreciation', 'NCAPRating', 'FueltankCapacitylitres', 'defects', 'car_state_MH', 'car_state_GJ'}\n",
      "\n",
      "{'Lengthmm', 'content.year', 'DriverSeatAdjustmentElectric_1', 'Right Rear Tyre_WARN', 'car_state_DL', 'content.transmission_Manual', 'EntertainmentDisplayScreenSizein', 'car_state_KA', 'SmartCardSmartKey_1', 'MaxPowerbhp', 'content.duplicateKey_1', 'ParkingAssistRear_Sensor & Camera', 'content.ownerNumber', 'Right Front Tyre_WARN', 'content.insuranceExpiry_months_remaining', 'MultifunctionDisplayScreenSizein', 'RimTypeFrontWheels_Steel', 'Bootspacelitres', 'Widthmm', 'Left Front Tyre_WARN', 'MaxTorqueNm', 'AmbientLighting_1', 'GearBoxNumberOfGears', 'SunroofMoonroof_1', 'SeatingCapacity', 'Left Rear Tyre_WARN', 'content.odometerReading', 'MaxPowerrpm', 'Airbags', 'Displacementcc', 'repainted', 'content.fitnessUpto_months_remaining', 'content.insuranceType_Comprehensive', 'Spare Tyre_WARN', 'SunroofType_nan', 'NumberOfSpeakers', 'car_state_HR', 'WheelBasemm', 'car_state_TN', 'NumberOfDiscBrakes', 'content.bodyType_SUV', 'SeatUpholstery_Synthetic Leather', 'HeadlampLensType_Projector Beam', 'content.lastServicedAt_months_remaining', 'GroundClearancemm', 'content.insuranceType_Zero Depreciation', 'NCAPRating', 'FueltankCapacitylitres', 'defects', 'car_state_MH', 'car_state_GJ'}\n"
     ]
    }
   ],
   "source": [
    "print(union_features)\n",
    "print()\n",
    "print(top_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "846d57a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Lengthmm', 'content.year', 'DriverSeatAdjustmentElectric_1', 'Right Rear Tyre_WARN', 'car_state_DL', 'content.transmission_Manual', 'EntertainmentDisplayScreenSizein', 'car_state_KA', 'SmartCardSmartKey_1', 'MaxPowerbhp', 'content.duplicateKey_1', 'ParkingAssistRear_Sensor & Camera', 'content.ownerNumber', 'Right Front Tyre_WARN', 'content.insuranceExpiry_months_remaining', 'MultifunctionDisplayScreenSizein', 'RimTypeFrontWheels_Steel', 'Bootspacelitres', 'Widthmm', 'FueltankCapacitylitres', 'Left Front Tyre_WARN', 'MaxTorqueNm', 'AmbientLighting_1', 'GearBoxNumberOfGears', 'SunroofMoonroof_1', 'SeatingCapacity', 'content.odometerReading', 'MaxPowerrpm', 'Airbags', 'Displacementcc', 'repainted', 'content.fitnessUpto_months_remaining', 'content.insuranceType_Comprehensive', 'Spare Tyre_WARN', 'NumberOfSpeakers', 'car_state_HR', 'WheelBasemm', 'car_state_TN', 'NumberOfDiscBrakes', 'content.bodyType_SUV', 'SeatUpholstery_Synthetic Leather', 'HeadlampLensType_Projector Beam', 'content.lastServicedAt_months_remaining', 'GroundClearancemm', 'content.insuranceType_Zero Depreciation', 'car_state_GJ', 'NCAPRating', 'Left Rear Tyre_WARN', 'defects', 'car_state_MH', 'SunroofType_nan'}\n"
     ]
    }
   ],
   "source": [
    "union_features_set = set(union_features)\n",
    "top_n_set = set(top_n)\n",
    "common_features = union_features_set.intersection(top_n_set)\n",
    "\n",
    "print(common_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "7b25e8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "common_features_lis = ['Lengthmm', 'content.year', 'DriverSeatAdjustmentElectric_1', 'Right Rear Tyre_WARN', 'car_state_DL', 'content.transmission_Manual', 'EntertainmentDisplayScreenSizein','SmartCardSmartKey_1', 'MaxPowerbhp', 'content.duplicateKey_1', 'ParkingAssistRear_Sensor & Camera', 'content.ownerNumber', 'Right Front Tyre_WARN', 'content.insuranceExpiry_months_remaining', 'MultifunctionDisplayScreenSizein', 'RimTypeFrontWheels_Steel', 'Bootspacelitres', 'Widthmm', 'FueltankCapacitylitres', 'Left Front Tyre_WARN', 'MaxTorqueNm', 'AmbientLighting_1', 'GearBoxNumberOfGears', 'SunroofMoonroof_1', 'SeatingCapacity', 'content.odometerReading', 'MaxPowerrpm', 'Airbags', 'Displacementcc', 'repainted', 'content.fitnessUpto_months_remaining', 'content.insuranceType_Comprehensive', 'Spare Tyre_WARN', 'NumberOfSpeakers', 'WheelBasemm','NumberOfDiscBrakes', 'content.bodyType_SUV', 'SeatUpholstery_Synthetic Leather', 'HeadlampLensType_Projector Beam', 'content.lastServicedAt_months_remaining', 'GroundClearancemm', 'content.insuranceType_Zero Depreciation','NCAPRating', 'Left Rear Tyre_WARN', 'defects', 'SunroofType_nan']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "8645e67c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(common_features_lis)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec6e7069",
   "metadata": {},
   "source": [
    "### I have realized that keeping these many columns will not make a general model which can work on any data, Instead I should be more focused on general features which are mostly available on all website & train model on those columns --- NEXT: figure out those columns which are very common among most of websites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f9334",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
